{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894ccac",
   "metadata": {},
   "source": [
    "# Data Manipulation Studio\n",
    "\n",
    "For this studio, we will revisit the data set from our last studio. If you recall, California farmers were looking for advice on growing pumpkins. We will use the same [pumpkins dataset](https://www.kaggle.com/usda/a-year-of-pumpkin-prices) as provided by the U.S. Department of Agriculture. You may have to clean data in the process of data manipulation, so feel free to pull up your notebook from the last class's studio.\n",
    "\n",
    "We will now be focusing our attention on a different region in the United States, the Northeast. When you open up the `dataset` folder, you will have 13 CSVs, including the San Francisco and Los Angeles data from the last lesson. The 13 CSVs are each a different terminal market in the United States.\n",
    "\n",
    "A **terminal market** is a central site, often in a metropolitan area, that serves as an assembly and trading place for commodities. Terminal markets for agricultural commodities are usually at or near major transportation hubs. [Definition Source](https://en.wikipedia.org/wiki/Terminal_market#:~:text=A%20terminal%20market%20is%20a,or%20near%20major%20transportation%20hubs)\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Import the CSVs for each of the following cities: Baltimore, Boston, New York, and Philadelphia. Set up a dataframe for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c06965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chriswright/Documents/LC2/data-analysis-projects/data-manipulation/studio/data-manipulation-studio\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c9a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and CSVs. Make some dataframes!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "baltimore = pd.read_csv(\"dataset/baltimore_9-24-2016_9-30-2017.csv\")\n",
    "boston = pd.read_csv(\"dataset/boston_9-24-2016_9-30-2017.csv\")\n",
    "nyc = pd.read_csv(\"dataset/new-york_9-24-2016_9-30-2017.csv\")\n",
    "philadelphia = pd.read_csv(\"dataset/philadelphia_9-24-2016_9-30-2017.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfda42f",
   "metadata": {},
   "source": [
    "## Clean Your Data\n",
    "\n",
    "In the last lesson, we cleaned the data related to San Francisco. Pull up your notebook from the last lesson and use it as a reference to clean up these new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98abc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean your data here!\n",
    "\"\"\" baltimore.info()\n",
    "baltimore.head()\n",
    "baltimore.describe()\n",
    "boston.info()\n",
    "boston.head()\n",
    "boston.describe()\n",
    "nyc.info()\n",
    "nyc.head()\n",
    "nyc.describe()\n",
    "philadelphia.info()\n",
    "philadelphia.head()\n",
    "philadelphia.describe() \"\"\"\n",
    "\n",
    "for df in [baltimore, boston, nyc, philadelphia]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "for df in [baltimore, boston, nyc, philadelphia]:\n",
    "    df.dropna (axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "for df in [baltimore, boston, nyc, philadelphia]:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "\n",
    "categorical_cols = baltimore.select_dtypes(include=\"object\").columns\n",
    "baltimore[categorical_cols] = baltimore[categorical_cols].fillna(\"Unknown\")\n",
    "\n",
    "categorical_cols = boston.select_dtypes(include=\"object\").columns\n",
    "boston[categorical_cols] = boston[categorical_cols].fillna(\"Unknown\")\n",
    "\n",
    "categorical_cols = nyc.select_dtypes(include=\"object\").columns\n",
    "nyc[categorical_cols] = nyc[categorical_cols].fillna(\"Unknown\")\n",
    "\n",
    "categorical_cols = philadelphia.select_dtypes(include=\"object\").columns\n",
    "philadelphia[categorical_cols] = philadelphia[categorical_cols].fillna(\"Unknown\")\n",
    "\n",
    "for df in [baltimore, boston, nyc, philadelphia]:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b01b9",
   "metadata": {},
   "source": [
    "## Combine Your Data\n",
    "\n",
    "Now that you have four clean sets of data, combine all four into one dataframe that represents the entire Northeast region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da059f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    commodity_name     city_name             package      variety sub_variety  \\\n",
      "0         PUMPKINS     BALTIMORE        24 inch bins      Unknown     Unknown   \n",
      "1         PUMPKINS     BALTIMORE        24 inch bins      Unknown     Unknown   \n",
      "2         PUMPKINS     BALTIMORE        24 inch bins  HOWDEN TYPE     Unknown   \n",
      "3         PUMPKINS     BALTIMORE        24 inch bins  HOWDEN TYPE     Unknown   \n",
      "4         PUMPKINS     BALTIMORE        24 inch bins  HOWDEN TYPE     Unknown   \n",
      "..             ...           ...                 ...          ...         ...   \n",
      "669       PUMPKINS  PHILADELPHIA  1/2 bushel cartons    MINIATURE   FLAT TYPE   \n",
      "670       PUMPKINS  PHILADELPHIA  1/2 bushel cartons    MINIATURE   FLAT TYPE   \n",
      "671       PUMPKINS  PHILADELPHIA  1/2 bushel cartons    MINIATURE   FLAT TYPE   \n",
      "672       PUMPKINS  PHILADELPHIA  1/2 bushel cartons    MINIATURE   FLAT TYPE   \n",
      "673       PUMPKINS  PHILADELPHIA  1/2 bushel cartons    MINIATURE   FLAT TYPE   \n",
      "\n",
      "          date  low_price  high_price  mostly_low  mostly_high    origin  \\\n",
      "0   2017-04-29        270       280.0         270        280.0   Unknown   \n",
      "1   2017-05-06        270       280.0         270        280.0   Unknown   \n",
      "2   2016-09-24        160       160.0         160        160.0  DELAWARE   \n",
      "3   2016-09-24        160       160.0         160        160.0  VIRGINIA   \n",
      "4   2016-11-05         90       100.0          90        100.0  MARYLAND   \n",
      "..         ...        ...         ...         ...          ...       ...   \n",
      "669 2016-11-05         16        18.0          16         18.0      OHIO   \n",
      "670 2017-08-26         18        20.0          18         20.0  MICHIGAN   \n",
      "671 2017-09-16         16        16.0          16         16.0      OHIO   \n",
      "672 2017-09-23         15        16.0          15         16.0      OHIO   \n",
      "673 2017-09-30         15        18.0          15         18.0      OHIO   \n",
      "\n",
      "    item_size    color unit_of_sale repack origin_district  \n",
      "0         lge  Unknown      Unknown      E             NaN  \n",
      "1         lge  Unknown      Unknown      E             NaN  \n",
      "2         med  Unknown      Unknown      N             NaN  \n",
      "3         med  Unknown      Unknown      N             NaN  \n",
      "4         lge  Unknown      Unknown      N             NaN  \n",
      "..        ...      ...          ...    ...             ...  \n",
      "669   Unknown      NaN   SHELLACKED      N             NaN  \n",
      "670   Unknown      NaN   SHELLACKED      N             NaN  \n",
      "671   Unknown      NaN   SHELLACKED      N             NaN  \n",
      "672   Unknown      NaN   SHELLACKED      N             NaN  \n",
      "673   Unknown      NaN   SHELLACKED      N             NaN  \n",
      "\n",
      "[674 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine the four dataframes into one!\n",
    "cities = [baltimore, boston, nyc, philadelphia]\n",
    "northeast_df = pd.concat(cities, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "print(northeast_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590082f",
   "metadata": {},
   "source": [
    "## Answer Some Questions\n",
    "\n",
    "Use `groupby()` and `agg()` to answer the following two questions:\n",
    "\n",
    "1. What is the mean low and high prices for each type of **unit of sale** in the Northeast region? \n",
    "2. For each region, what is the average number of pumpkins per variety that came into terminal markets for the year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c839639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  unit_of_sale  mean_low_price  mean_high_price\n",
      "0         EACH       47.916667        59.166667\n",
      "1      PER BIN      185.845070       206.619718\n",
      "2   SHELLACKED       16.000000        17.545455\n",
      "3      Unknown      133.660211       150.122359\n"
     ]
    }
   ],
   "source": [
    "# Put your code here to find the mean low and high prices in the Northeast region for each type of unit of sale.\n",
    "\n",
    "mean_prices = northeast_df.groupby(\"unit_of_sale\").agg(\n",
    "    mean_low_price = (\"low_price\", \"mean\"),\n",
    "    mean_high_price = (\"high_price\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "print(mean_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4b23352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     variety  count\n",
      "0              BIG MACK TYPE     55\n",
      "1                  BLUE TYPE      7\n",
      "2                 CINDERELLA     39\n",
      "3                  FAIRYTALE     37\n",
      "4                HOWDEN TYPE    224\n",
      "5          HOWDEN WHITE TYPE      2\n",
      "6               KNUCKLE HEAD      9\n",
      "7                  MINIATURE     97\n",
      "8   MIXED HEIRLOOM VARIETIES      4\n",
      "9                   PIE TYPE    198\n",
      "10                   Unknown      2\n"
     ]
    }
   ],
   "source": [
    "# Put your code here to find the average number of pumpkins coming into terminal markets of each variety.\n",
    "avg_pumpkins = northeast_df.groupby(\"variety\").size().reset_index(name=\"count\")\n",
    "print(avg_pumpkins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5cff4",
   "metadata": {},
   "source": [
    "## Bonus Mission\n",
    "\n",
    "Try answering the same questions for the Midwest (Chicago, Detroit, and St. Louis) or the Southeast (Atlanta, Columbia, and Miami) regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d22b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the bonus mission if you have time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbc152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 LC2",
   "language": "python",
   "name": "lc2-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
